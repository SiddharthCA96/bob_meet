ğŸ“˜ BOG Meeting RAG-Based Ordinance Chatbot

A Retrieval-Augmented Generation (RAG) powered chatbot designed to answer queries related to Board of Governors (BOG) meeting ordinances, summaries, and institutional documents.
The system uses vector embeddings, graph-based knowledge representation, and LLM-powered reasoning to provide accurate, citation-backed responses.

ğŸ“ Project Structure
BOG_MEETING_CHATBOT/
â”‚
â”œâ”€â”€ logs/                         # System logs & debug files
â”œâ”€â”€ summaries/                    # Generated summaries of ordinances/BOG documents
â”œâ”€â”€ vector_store/                 # Saved embeddings & vector index
â”‚
â”œâ”€â”€ chatbot_ui.py                 # Chat UI (Streamlit/Gradio) for user interaction
â”œâ”€â”€ create_graph_database.py      # Creates graph/knowledge DB (Neo4j / NetworkX)
â”œâ”€â”€ create_vector_embedding.py    # PDF/text chunking + embedding creation pipeline
â”‚
â”œâ”€â”€ rag_query_handler.py          # Main RAG pipeline (retrieval + generation)
â”œâ”€â”€ rag_debug.log                 # Debug logs for RAG pipeline
â”œâ”€â”€ debug.log                     # General system logs
â”‚
â”œâ”€â”€ Pipfile                       # Pipenv environment file
â”œâ”€â”€ Pipfile.lock                  # Pipenv lock
â”œâ”€â”€ requirement.txt               # Requirements for pip users
â”œâ”€â”€ .gitignore                    # Git ignored paths
â””â”€â”€ README.md                     # Project documentation

ğŸš€ Features
ğŸ” Hybrid RAG Pipeline

Dense vector search (via embeddings)

Optional graph-based retrieval (relations between entities)

Combined context fed to LLM for grounded answers

ğŸ§  LLM-Driven Response Generation

Uses Groq/OpenAI/Llama models (depending on configuration):

Generates precise answers

Includes fallback logic for unclear queries

Avoids hallucination by grounding responses in documents

ğŸ“„ Document Processing

Processes BOG meeting ordinances

Summarizes key sections

Converts PDF â†’ text â†’ chunks â†’ embeddings

ğŸ’¬ Interactive Chat Interface

Simple UI (Streamlit or Gradio)

User query history

Debug info (optional)

âš™ï¸ Installation
1. Clone the repository
git clone https://github.com/<your-username>/BOG_MEETING_CHATBOT.git
cd BOG_MEETING_CHATBOT

2. Install dependencies
Using Pip:
pip install -r requirement.txt

OR using Pipenv:
pipenv install
pipenv shell

ğŸ”‘ Environment Variables

Create a .env in the project root:

GROQ_API_KEY=your_api_key_here
OPENAI_API_KEY=your_optional_openai_key


Never hard-code API keys inside Python files.

ğŸ—ï¸ Setup: Build Vector Embeddings

Before running the chatbot, generate embeddings:

python create_vector_embedding.py


This will:

âœ” Load PDFs / text
âœ” Chunk data
âœ” Create embeddings
âœ” Save them in vector_store/

ğŸ§ª Optional: Build Graph Knowledge Base

Run this only if you're using graph-augmented RAG:

python create_graph_database.py

â–¶ï¸ Run the Chatbot UI
python chatbot_ui.py


Then open the local URL (e.g., http://localhost:8501).

ğŸ§  How RAG Works in This Project

User enters a query.

System retrieves top relevant chunks from vector_store.

Graph DB supplements data with relationships if enabled.

LLM generates an answer grounded in retrieved info.

System logs detailed steps in rag_debug.log.

ğŸ“š Use Cases

Query BOG meeting ordinances

Understand rules, decisions, and resolutions

Summaries of past meeting notes

Assist administration, students, and faculty

â­ Future Enhancements

Web deployment (Vercel/Render)

Improved summarizer using Llama/Groq large models

Admin panel for uploading new meeting documents

SQL vector store integration

Chat history memory
